{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"UvZKRQJkazb3"},"source":["# Ejercicio1, Regresion Logistica One vs All\n","\n","Nombre: Quispe Taboada Daniel <br>\n","Dataset: ['White Wine Quality'](https://www.kaggle.com/datasets/piyushagni5/white-wine-quality)"]},{"cell_type":"code","execution_count":416,"metadata":{"executionInfo":{"elapsed":404,"status":"ok","timestamp":1671224779431,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"VOZrDdqUazb8"},"outputs":[],"source":["# Cálculo científico y vectorial para python\n","import numpy as np\n","import pandas as pd\n","import random as rd\n","\n","from matplotlib import pyplot # Libreria para graficos\n","from scipy import optimize # Modulo de optimizacion en scipy\n","\n","# le dice a matplotlib que incruste gráficos en el cuaderno\n","%matplotlib inline"]},{"cell_type":"code","execution_count":417,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2547,"status":"ok","timestamp":1671224782395,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"kCJgRWVsbejc","outputId":"1fbbfe19-6aa3-4b34-cd70-c3d7d735fe38"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# para trabaja en colab\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":418,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671224782395,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"yhonGSZmazb9"},"outputs":[],"source":["# Para colab\n","# dataN = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/SegundoPRacial/Data/winequality-white.csv', sep=';')\n","dataN = pd.read_csv('Data/winequality-white.csv', sep=';')\n","data = dataN.to_numpy()"]},{"cell_type":"code","execution_count":419,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671224782395,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"s_ZsJsNYazb-"},"outputs":[],"source":["num_labels = 10 \n","X, y = data[:, 0:11], data[:, 11].ravel() \n","m, n = X.shape "]},{"cell_type":"code","execution_count":420,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671224782396,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"ThPH_9tCcpP3"},"outputs":[],"source":["limite = 10000-m\n","i = 0\n","xSinteticos = []\n","ySinteticos = []\n","completo = False\n","while not(completo):\n","    if i == m:\n","      i = 0\n","    evaluar = rd.randrange(0, 2)\n","    if evaluar == 0:\n","      aumentar = 1.1\n","    else: \n","      evaluar = 0.9\n","    registroX = []\n","    registroY = y[i]\n","    for j in range(11):\n","      valorX = X[i][j]*aumentar\n","      registroX.append(valorX)\n","    ySinteticos.append(registroY)\n","    xSinteticos.append(registroX)\n","    i = i + 1\n","    if len(xSinteticos) == limite:\n","      completo = True\n","X = np.concatenate((X, xSinteticos), axis=0, out=None)\n","y = np.append(y, ySinteticos)"]},{"cell_type":"code","execution_count":421,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671224782396,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"H6SU5qb1sBNn"},"outputs":[],"source":["m, n = X.shape"]},{"cell_type":"code","execution_count":422,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671224782396,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"ofCnKKQIcH2M"},"outputs":[],"source":["def  featureNormalize(X):\n","    X_norm = X.copy()\n","    mu = np.zeros(X.shape[1])\n","    sigma = np.zeros(X.shape[1])\n","\n","    mu = np.mean(X, axis = 0)\n","    sigma = np.std(X, axis = 0)\n","    X_norm = (X - mu) / sigma\n","    \n","    return X_norm, mu, sigma"]},{"cell_type":"code","execution_count":423,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671224782397,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"58NQ9XGxcS51"},"outputs":[],"source":["X, mu, sigma = featureNormalize(X)\n"]},{"cell_type":"code","execution_count":424,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671224782397,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"9dJwdMRHazb-"},"outputs":[],"source":["# valores de prueba para los parámetros theta\n","theta_t = np.array([-3, -1, 1, 3], dtype=float)\n","\n","# valores de prueba para las entradas\n","X_t = np.concatenate([np.ones((5, 1)), np.arange(1, 16).reshape(5, 3, order='F')/10.0], axis=1)\n","# valores de testeo para las etiquetas\n","y_t = np.array([1, 0, 1, 0, 1])\n","# valores de testeo para el parametro de regularizacion\n","lambda_t = 1"]},{"cell_type":"code","execution_count":425,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671224782397,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"sQYqxBfpazb_"},"outputs":[],"source":["def sigmoid(z):\n","    \"\"\"\n","    Calcula la sigmoide de z.\n","    \"\"\"\n","    return 1.0 / (1.0 + np.exp(-z))"]},{"cell_type":"code","execution_count":426,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1671224782397,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"w-uRwPp5azb_"},"outputs":[],"source":["def lrCostFunction(theta, X, y, lambda_):\n","    \"\"\"\n","    Calcula el costo de usar theta como parámetro para la regresión logística regularizada y \n","    el gradiente del costo w.r.t. a los parámetros.\n","    \n","    Parametros\n","    ----------\n","    theta : array_like\n","        Parametro theta de la regresion logistica. Vector de la forma(shape) (n, ). n es el numero de caracteristicas \n","        incluida la intercepcion\n","        \n","    X : array_like\n","        Dataset con la forma(shape) (m x n). m es el numero de ejemplos, y n es el numero de \n","        caracteristicas (incluida la intercepcion).\n","    \n","    y : array_like\n","        El conjunto de etiquetas. Un vector con la forma (shape) (m, ). m es el numero de ejemplos\n","    \n","    lambda_ : float\n","        Parametro de regularización. \n","    \n","    Devuelve\n","    -------\n","    J : float\n","        El valor calculado para la funcion de costo regularizada. \n","    \n","    grad : array_like\n","        Un vector de la forma (shape) (n, ) que es el gradiente de la \n","        función de costo con respecto a theta, en los valores actuales de theta..\n","    \"\"\"\n","    # Inicializa algunos valores utiles\n","    m = y.size\n","    \n","    # convierte las etiquetas a valores enteros si son boleanos\n","    if y.dtype == bool:\n","        y = y.astype(int)\n","    \n","    J = 0\n","    grad = np.zeros(theta.shape)\n","    \n","    h = sigmoid(X.dot(theta.T))\n","    \n","    temp = theta\n","    temp[0] = 0\n","    \n","    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))\n","    \n","    grad = (1 / m) * (h - y).dot(X) \n","    grad = grad + (lambda_ / m) * temp\n","\n","    return J, grad"]},{"cell_type":"code","execution_count":427,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1671224782398,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"iPdisUqYazcA","outputId":"bee0f115-7e3e-4580-eec1-b6a128c6e0a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Costo: 1.894713\n","Gradientes: [0.193750, -0.132337, 0.364538, 0.861413]\n"]}],"source":["J, grad = lrCostFunction(theta_t, X_t, y_t, lambda_t)\n","\n","print('Costo: {:.6f}'.format(J))\n","print('Gradientes: [{:.6f}, {:.6f}, {:.6f}, {:.6f}]'.format(*grad))"]},{"cell_type":"code","execution_count":428,"metadata":{"executionInfo":{"elapsed":425,"status":"ok","timestamp":1671224782815,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"7xyjzsmeazcB"},"outputs":[],"source":["def oneVsAll(X, y, num_labels, lambda_):\n","    \"\"\"\n","    Trains num_labels logistic regression classifiers and returns\n","    each of these classifiers in a matrix all_theta, where the i-th\n","    row of all_theta corresponds to the classifier for label i.\n","    \n","    Parameters\n","    ----------\n","    X : array_like\n","        The input dataset of shape (m x n). m is the number of \n","        data points, and n is the number of features. Note that we \n","        do not assume that the intercept term (or bias) is in X, however\n","        we provide the code below to add the bias term to X. \n","    \n","    y : array_like\n","        The data labels. A vector of shape (m, ).\n","    \n","    num_labels : int\n","        Number of possible labels.\n","    \n","    lambda_ : float\n","        The logistic regularization parameter.\n","    \n","    Returns\n","    -------\n","    all_theta : array_like\n","        The trained parameters for logistic regression for each class.\n","        This is a matrix of shape (K x n+1) where K is number of classes\n","        (ie. `numlabels`) and n is number of features without the bias.\n","    \"\"\"\n","    # algunas variables utiles\n","    m, n = X.shape\n","    \n","    all_theta = np.zeros((num_labels, n + 1))\n","\n","    # Agrega unos a la matriz X\n","    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n","\n","    for c in np.arange(num_labels):\n","        initial_theta = np.zeros(n + 1)\n","        options = {'maxiter': 1000}\n","        res = optimize.minimize(lrCostFunction, \n","                                initial_theta, \n","                                (X, (y == c), lambda_), \n","                                jac=True, \n","                                method='CG',\n","                                options=options) \n","        \n","        all_theta[c] = res.x\n","\n","    return all_theta"]},{"cell_type":"code","execution_count":429,"metadata":{"executionInfo":{"elapsed":1052,"status":"ok","timestamp":1671224783865,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"dLK1wpcFazcC"},"outputs":[],"source":["lambda_ = 0.1\n","all_theta = oneVsAll(X, y, num_labels, lambda_)"]},{"cell_type":"code","execution_count":430,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671224783865,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"VVIvmhgWazcD"},"outputs":[],"source":["def predictOneVsAll(all_theta, X):\n","    \"\"\"\n","    Devuelve un vector de predicciones para cada ejemplo en la matriz X.\n","    Tenga en cuenta que X contiene los ejemplos en filas. \n","    all_theta es una matriz donde la i-ésima fila es un vector theta de regresión logística entrenada para la i-ésima clase. \n","    Debe establecer p en un vector de valores de 0..K-1 (por ejemplo, p = [0, 2, 0, 1] \n","    predice clases 0, 2, 0, 1 para 4 ejemplos).\n","    \n","    Parametros\n","    ----------\n","    all_theta : array_like\n","        The trained parameters for logistic regression for each class.\n","        This is a matrix of shape (K x n+1) where K is number of classes\n","        and n is number of features without the bias.\n","    \n","    X : array_like\n","        Data points to predict their labels. This is a matrix of shape \n","        (m x n) where m is number of data points to predict, and n is number \n","        of features without the bias term. Note we add the bias term for X in \n","        this function. \n","    \n","    Devuelve\n","    -------\n","    p : array_like\n","        The predictions for each data point in X. This is a vector of shape (m, ).\n","    \"\"\"\n","    \n","    m = X.shape[0];\n","    num_labels = all_theta.shape[0]\n","\n","    p = np.zeros(m)\n","\n","    # Add ones to the X data matrix\n","    X = np.concatenate([np.ones((m, 1)), X], axis=1)\n","    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1)\n","\n","    return p"]},{"cell_type":"code","execution_count":431,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671224783866,"user":{"displayName":"Daniel Quispe Taboada","userId":"05940309157313605164"},"user_tz":240},"id":"OGw8jytKazcD","outputId":"f83fb8dd-f1ea-4513-c936-d15c213b8adf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Precision del conjuto de entrenamiento: 53.78%\n","5.0\n","Los datos del vino para evaluar es: \n","[[ 1.         -1.3757054   0.72312858 -1.95669098 -0.9575346   0.37703888\n","   0.10116667 -0.3885043  -1.05125204 -0.4928875  -0.28137582 -0.67468963]] \n"," La calidad del vino es de 5\n"]}],"source":["pred = predictOneVsAll(all_theta, X)\n","print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))\n","nPrueba = 561\n","XPrueba = X[nPrueba:nPrueba+1, :].copy()\n","print(y[nPrueba])\n","XPrueba = np.concatenate([np.ones((1, 1)), XPrueba], axis=1)\n","p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis = 1)\n","print('Los datos del vino para evaluar es: ')\n","print(XPrueba, '\\n La calidad del vino es de',int(p))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"db770a79811cf06dd2eb2d8661a865789fd9658586cf1984ee660fb686aab52b"}}},"nbformat":4,"nbformat_minor":0}
